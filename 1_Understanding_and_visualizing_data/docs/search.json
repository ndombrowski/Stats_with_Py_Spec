[
  {
    "objectID": "code/1_intro.html",
    "href": "code/1_intro.html",
    "title": "2  Lecture notes",
    "section": "",
    "text": "A great resource that you can explore is the This is Statistics website, created by the American Statistical Association. This insightful and motivating campaign has countless links, videos, and resources to raise awareness of the wide variety of fascinating careers within statistics."
  },
  {
    "objectID": "code/1_intro.html#definitions",
    "href": "code/1_intro.html#definitions",
    "title": "2  Lecture notes",
    "section": "2.1 Definitions",
    "text": "2.1 Definitions\n\nThe mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set. Its highly affected by outliers.\nThe median is the middle value when a data set is ordered from least to greatest.\nThe mode is the number that occurs most often in a data set.\nRange: the difference between the highest and lowest values.\nInterquartile range: the range of the middle half of a distribution. Q3-Q3\nStandard deviation: average distance from the mean."
  },
  {
    "objectID": "code/1_intro.html#standard-score-empirical-rule",
    "href": "code/1_intro.html#standard-score-empirical-rule",
    "title": "2  Lecture notes",
    "section": "2.2 Standard Score (Empirical Rule)",
    "text": "2.2 Standard Score (Empirical Rule)\nA bell-shaped or normal distributions is sometimes referred to as the 68-95-99.7 rule: 68% of the population is within 1 standard deviation of the mean. 95% of the population is within 2 standard deviation of the mean. 99.7% of the population is within 3 standard deviation of the mean."
  },
  {
    "objectID": "code/2_introduction_jupyter.html",
    "href": "code/2_introduction_jupyter.html",
    "title": "3  What is Jupyter Notebooks?",
    "section": "",
    "text": "Jupyter is a web-based interactive development environment that supports multiple programming languages, however most commonly used with the Python programming language.\nThe interactive environment that Jupyter provides enables students, scientists, and researchers to create reproducible analysis and formulate a story within a single document.\nLets take a look at an example of a completed Jupyter Notebook: Example Notebook"
  },
  {
    "objectID": "code/2_introduction_jupyter.html#h2",
    "href": "code/2_introduction_jupyter.html#h2",
    "title": "3  What is Jupyter Notebooks?",
    "section": "4.1 H2",
    "text": "4.1 H2\n\n4.1.1 H3\n\n4.1.1.1 H4\n\n4.1.1.1.1 H5\n\n4.1.1.1.1.1 H6\nText modifications:\nEmphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLists:\n\nFirst ordered list item\nAnother item\n\n\nUnordered sub-list.\n\n\nActual numbers don’t matter, just that it’s a number\nOrdered sub-list\nAnd another item.\n\n\nUnordered list can use asterisks\nOr minuses\nOr pluses\n\nLinks:\nhttp://www.umich.edu\nhttp://www.umich.edu\nThe University of Michigan’s Homepage\nTo look into more examples of Markdown syntax and features such as tables, images, etc. head to the following link: Markdown Reference\n\n\n\n\n\n4.1.2 Kernels, Variables, and Environment\nA notebook kernel is a “computational engine” that executes the code contained in a Notebook document. There are kernels for various programming languages, however we are solely using the python kernel which executes python code.\nWhen a notebook is opened, the associated kernel is automatically launched for our convenience.\n\n### This is python\nprint(\"This is a python code cell\")\n\nThis is a python code cell\n\n\nA kernel is the back-end of our notebook which not only executes our python code, but stores our initialized variables.\n\n### For example, lets initialize variable x\n\nx = 1738\n\nprint(\"x has been set to \" + str(x))\n\nx has been set to 1738\n\n\n\n### Print x\n\nprint(x)\n\n1738\n\n\nIssues arrise when we restart our kernel and attempt to run code with variables that have not been reinitialized.\nIf the kernel is reset, make sure to rerun code where variables are intialized.\n\n## We can also run code that accepts input\n\nname = input(\"What is your name? \")\n\nprint(\"The name you entered is \" + name)\n\nIt is important to note that Jupyter Notebooks have in-line cell execution. This means that a prior executing cell must complete its operations prior to another cell being executed. A cell still being executing is indicated by the [*] on the left-hand side of the cell.\n\nprint(\"This won't print until all prior cells have finished executing.\")\n\n\n\n4.1.3 Command vs. Edit Mode & Shortcuts\nThere is an edit and a command mode for jupyter notebooks. The mode is easily identifiable by the color of the left border of the cell.\nBlue = Command Mode.\nGreen = Edit Mode.\nCommand Mode can be toggled by pressing esc on your keyboard.\nCommands can be used to execute notebook functions. For example, changing the format of a markdown cell or adding line numbers.\nLets toggle line numbers while in command mode by pressing L.\n\n4.1.3.1 Additional Shortcuts\nThere are a lot of shortcuts that can be used to improve productivity while using Jupyter Notebooks.\nHere is a list:\n\n\n\nJupyter Notebook Shortcuts\n\n\n\n\n\n4.1.4 How do you install Jupyter Notebooks?\nNote: Coursera provides embedded jupyter notebooks within the course, thus the download is not a requirement unless you wish to explore jupyter further on your own computer.\nOfficial Installation Guide: https://jupyter.readthedocs.io/en/latest/install.html\nJupyter recommends utilizing Anaconda, which is a platform compatible with Windows, macOS, and Linux systems.\nAnaconda Download: https://www.anaconda.com/download/#macos"
  },
  {
    "objectID": "code/3_data_types.html",
    "href": "code/3_data_types.html",
    "title": "4  Data Types in Python",
    "section": "",
    "text": "We will only focus on the bolded ones\nLet’s connect these data types to the the variable types we learned from the Variable Types video.\n\n4.0.1 Numerical or Quantitative (taking the mean makes sense)\n\nDiscrete\n\nInteger (int) #Stored exactly, i.e. a whole number\n\nContinuous\n\nFloat (float) #Stored similarly to scientific notation. Allows for decimal places but loses precision.\n\n\n\nimport math\n\n\n#the type function tells us with what data type we are working\ntype(4)\n\nint\n\n\n\ntype(0)\n\nint\n\n\n\ntype(-3)\n\nint\n\n\n\n#try taking the mean\nnumbers = [2, 3, 4, 5]\nprint(sum(numbers)/len(numbers))\ntype(sum(numbers)/len(numbers)) #In Python 3 returns float, but in Python 2 would return int\n\n3.5\n\n\nfloat\n\n\nFloats\n\n3/5\n\n0.6\n\n\n\n6*10**(-1)\n\n0.6000000000000001\n\n\n\ntype(3/5)\n\nfloat\n\n\n\ntype(math.pi)\n\nfloat\n\n\n\ntype(4.0)\n\nfloat\n\n\n\n# Try taking the mean\nnumbers = [math.pi, 3/5, 4.1]\ntype(sum(numbers)/len(numbers))\n\nfloat\n\n\n\n\n4.0.2 Categorical or Qualitative\n\nNominal\n\nBoolean (bool)\nString (str)\nNone (NoneType)\n\nOrdinal\n\nOnly defined by how you use the data\nOften important when creating visuals\nLists can hold ordinal information because they have indices\n\n\nBoolean\nBooleans essentially are stored as True or False. I.e. below we see that True is a reservered word in python.\n\n# Boolean\ntype(True)\n\nbool\n\n\nWe also can make our own booleans:\n\ntype(bool('yes'))\n\nbool\n\n\nWe can also use booleans in if statements, i.e. If the below is true, print something.\n\n# Boolean\nif 4 < 5:\n    print(\"Yes!\")\n\nYes!\n\n\n\nmyList = [True, 6<5, 1==3, None is None]\nfor element in myList:\n    print(type(element))\n\n<class 'bool'>\n<class 'bool'>\n<class 'bool'>\n<class 'bool'>\n\n\nFor booleans, True equals to the value of 1 and False to the value of 0. This is why we can do math with booleans. Below we get a value of 0.5, since half of the statements above are true.\n\nprint(sum(myList)/len(myList))\ntype(sum(myList)/len(myList))\n\n0.5\n\n\nfloat\n\n\nString\n\ntype(\"This sentence makes sense\")\n\nstr\n\n\n\ntype(\"Makes sentense this sense\")\n\nstr\n\n\n\ntype(\"math.pi\")\n\nstr\n\n\n\nstrList = ['dog', 'koala', 'goose']\n\n#the code below gives an error because we can not calculate the mean on a string\n#sum(strList)/len(strList)\n\nNonetype\n\n# None\ntype(None)\n\nNoneType\n\n\n\n# None\nx = None\ntype(x)\n\nNoneType\n\n\n\nnoneList = [None]*5\n\n##the code below gives an error because we can not calculate the mean on a NoneType\n#sum(nonList)/len(nonList)\n\nLists\nA list can hold many types and can also be used to store ordinal information.\n\n# List\nmyList = [1, 1.1, \"This is a sentence\", None]\n\nfor element in myList:\n    print(type(element))\n\n<class 'int'>\n<class 'float'>\n<class 'str'>\n<class 'NoneType'>\n\n\n\n#this would give an error because the list contains categorical data\n#sum(myList)/len(myList)\n\n\n# List\nmyList = [1, 2, 3]\n\nfor element in myList:\n    print(type(element))\n\nsum(myList)/len(myList) # note that this outputs a float\n\n<class 'int'>\n<class 'int'>\n<class 'int'>\n\n\n2.0\n\n\nWhile we would see the order in the list below, by default Python does not see this as an ordinal category:\n\nmyList = ['third', 'first', 'medium', 'small', 'large']\n\n#use an index to access data\nmyList[0]\n\n'third'\n\n\n\nmyList.sort()\nmyList\n\n['first', 'large', 'medium', 'small', 'third']\n\n\nThere are more datatypes available when using different libraries such as Pandas and Numpy, which we will introduce to you as we use them."
  },
  {
    "objectID": "code/4_libraries_data_management.html",
    "href": "code/4_libraries_data_management.html",
    "title": "5  Python Libraries",
    "section": "",
    "text": "Python, like other programming languages, has an abundance of additional modules or libraries that augument the base framework and functionality of the language.\nThink of a library as a collection of functions that can be accessed to complete certain programming tasks without having to write your own algorithm.\nFor this course, we will focus primarily on the following libraries:"
  },
  {
    "objectID": "code/4_libraries_data_management.html#explore-what-datatypes-we-work-with-using-dtypes",
    "href": "code/4_libraries_data_management.html#explore-what-datatypes-we-work-with-using-dtypes",
    "title": "5  Python Libraries",
    "section": "7.1 Explore what datatypes we work with using dtypes",
    "text": "7.1 Explore what datatypes we work with using dtypes\nWe can view the data types of our data frame columns with by calling .dtypes on our data frame:\n\ndf.dtypes\n\nID                 int64\nAge                int64\nGender            object\nGenderGroup        int64\nGlasses           object\nGlassesGroup       int64\nHeight           float64\nWingspan         float64\nCWDistance         int64\nComplete          object\nCompleteGroup      int64\nScore              int64\ndtype: object\n\n\nThe output indicates we have integers, floats, and objects with our Data Frame."
  },
  {
    "objectID": "code/4_libraries_data_management.html#print-unique-values",
    "href": "code/4_libraries_data_management.html#print-unique-values",
    "title": "5  Python Libraries",
    "section": "7.2 Print unique values",
    "text": "7.2 Print unique values\nWe may also want to observe the different unique values within a specific column, lets do this for Gender:\n\n# List unique values in the df['Gender'] column\ndf.Gender.unique()\n\narray(['F', 'M'], dtype=object)\n\n\n\n# Lets explore df[\"GenderGroup] as well\ndf.GenderGroup.unique()\n\narray([1, 2])\n\n\nIt seems that these fields may serve the same purpose, which is to specify male vs. female. Lets check this quickly by observing only these two columns:\n\n# Use .loc() to specify a list of mulitple column names\ndf.loc[:,[\"Gender\", \"GenderGroup\"]]\n\n\n\n\n\n  \n    \n      \n      Gender\n      GenderGroup\n    \n  \n  \n    \n      0\n      F\n      1\n    \n    \n      1\n      F\n      1\n    \n    \n      2\n      F\n      1\n    \n    \n      3\n      F\n      1\n    \n    \n      4\n      M\n      2\n    \n    \n      5\n      M\n      2\n    \n    \n      6\n      M\n      2\n    \n    \n      7\n      F\n      1\n    \n    \n      8\n      M\n      2\n    \n    \n      9\n      F\n      1\n    \n    \n      10\n      M\n      2\n    \n    \n      11\n      F\n      1\n    \n    \n      12\n      F\n      1\n    \n    \n      13\n      F\n      1\n    \n    \n      14\n      M\n      2\n    \n    \n      15\n      M\n      2\n    \n    \n      16\n      F\n      1\n    \n    \n      17\n      M\n      2\n    \n    \n      18\n      M\n      2\n    \n    \n      19\n      F\n      1\n    \n    \n      20\n      M\n      2\n    \n    \n      21\n      M\n      2\n    \n    \n      22\n      M\n      2\n    \n    \n      23\n      M\n      2\n    \n    \n      24\n      F\n      1"
  },
  {
    "objectID": "code/4_libraries_data_management.html#summarizing-multiple-columns-using-groupby",
    "href": "code/4_libraries_data_management.html#summarizing-multiple-columns-using-groupby",
    "title": "5  Python Libraries",
    "section": "7.3 Summarizing multiple columns using groupby",
    "text": "7.3 Summarizing multiple columns using groupby\nFrom eyeballing the output, it seems to check out. We can streamline this by utilizing the groupby() and size() functions.\n\ndf.groupby(['Gender','GenderGroup']).size()\n\nGender  GenderGroup\nF       1              12\nM       2              13\ndtype: int64\n\n\nThis output indicates that we have two types of combinations.\n\nCase 1: Gender = F & Gender Group = 1\nCase 2: Gender = M & GenderGroup = 2.\n\nThis validates our initial assumption that these two fields essentially portray the same information."
  },
  {
    "objectID": "code/5_nhanes_data_basics.html",
    "href": "code/5_nhanes_data_basics.html",
    "title": "6  Using Python to read data files and explore their contents",
    "section": "",
    "text": "Note that Python by itself is a general-purpose programming language and does not provide high-level data processing capabilities. The Pandas library was developed to meet this need. Pandas is the most popular Python library for data manipulation, and we will use it extensively in this course.\nIn addition to Pandas, we will also make use of the following Python libraries\n\nNumpy is a library for working with arrays of data\nMatplotlib is a library for making graphs\nSeaborn is a higher-level interface to Matplotlib that can be used to simplify many graphing tasks\nStatsmodels is a library that implements many statistical techniques\nScipy is a library of techniques for numerical and scientific computing\n\n\n6.0.1 Importing libraries\nWhen using Python, you must always begin your scripts by importing the libraries that you will be using. After importing a library, its functions can then be called from your code by prepending the library name to the function name. For example, to use the ‘dot’ function from the ‘numpy’ library, you would enter ‘numpy.dot’. To avoid repeatedly having to type the libary name in your scripts, it is conventional to define a two or three letter abbreviation for each library, e.g. ‘numpy’ is usually abbreviated as ‘np’. This allows us to use ‘np.dot’ instead of ‘numpy.dot’. Similarly, the Pandas library is typically abbreviated as ‘pd’.\nThe following statement imports the Pandas library, and gives it the abbreviated name ‘pd’.\n\nimport pandas as pd\n\n\n\n6.0.2 Reading a data file\nWe will be working with the NHANES (National Health and Nutrition Examination Survey) data from the 2015-2016 wave, which has been discussed earlier in this course. The raw data for this study are available here:\nhttps://wwwn.cdc.gov/nchs/nhanes/Default.aspx\nAs in many large studies, the NHANES data are spread across multiple files. The NHANES files are stored in SAS transport (Xport) format. This is a somewhat obscure format, and while Pandas is perfectly capable of reading the NHANES data directly from the xport files, accomplishing this task is a more advanced topic than we want to get into here. Therefore, for this course we have prepared some merged datasets in text/csv format.\nPandas is a large and powerful library. Here we will only use a few of its basic features. The main data structure that Pandas works with is called a “data frame”. This is a two-dimensional table of data in which the rows typically represent cases (e.g. NHANES subjects), and the columns represent variables. Pandas also has a one-dimensional data structure called a Series that we will encounter occasionally.\nPandas has a variety of functions named with the pattern ‘read_xxx’ for reading data in different formats into Python. Right now we will focus on reading ‘csv’ files, so we are using the ‘read_csv’ function, which can read csv (and “tsv”) format files that are exported from spreadsheet software like Excel. The ‘read_csv’ function by default expects the first row of the data file to contain column names.\nUsing ‘read_csv’ in its default mode is fairly straightforward. There are many options to ‘read_csv’ that are useful for handling less-common situations. For example, you would use the option sep='\\t' instead of the default sep=',' if the fields of your data file are delimited by tabs instead of commas. See here for the full documentation for ‘read_csv’.\nPandas can read a data file over the internet when provided with a URL, which is what we will do below. In the Python script we will name the data set ‘da’, i.e. this is the name of the Python variable that will hold the data frame after we have loaded it.\nThe variable ‘url’ holds a string (text) value, which is the internet URL where the data are located. If you have the data file in your local filesystem, you can also use ‘read_csv’ to read the data from this file. In this case you would pass a file path instead of a URL, e.g. pd.read_csv(\"my_file.csv\") would read a file named my_file.csv that is located in your current working directory.\n\nurl = \"../data/nhanes_2015_2016.csv\"\nda = pd.read_csv(url)\n\nTo confirm that we have actually obtained the data the we are expecting, we can display the shape (number of rows and columns) of the data frame in the notebook. Note that the final expression in any Jupyter notebook cell is automatically printed, but you can force other expressions to be printed by using the ‘print’ function, e.g. ‘print(da.shape)’.\nBased on what we see below, the data set being read here has 5735 rows, corresponding to 5735 people in this wave of the NHANES study, and 28 columns, corresponding to 28 variables in this particular data file. Note that NHANES collects thousands of variables on each study subject, but here we are working with a reduced file that contains a limited number of variables.\n\nda.shape\n\n(5735, 28)\n\n\n\n\n6.0.3 Exploring the contents of a data set\nPandas has a number of basic ways to understand what is in a data set. For example, above we used the ‘shape’ method to determine the numbers of rows and columns in a data set. The columns in a Pandas data frame have names, to see the names, use the ‘columns’ method:\n\nda.columns\n\nIndex(['SEQN', 'ALQ101', 'ALQ110', 'ALQ130', 'SMQ020', 'RIAGENDR', 'RIDAGEYR',\n       'RIDRETH1', 'DMDCITZN', 'DMDEDUC2', 'DMDMARTL', 'DMDHHSIZ', 'WTINT2YR',\n       'SDMVPSU', 'SDMVSTRA', 'INDFMPIR', 'BPXSY1', 'BPXDI1', 'BPXSY2',\n       'BPXDI2', 'BMXWT', 'BMXHT', 'BMXBMI', 'BMXLEG', 'BMXARML', 'BMXARMC',\n       'BMXWAIST', 'HIQ210'],\n      dtype='object')\n\n\nThese names correspond to variables in the NHANES study. For example, SEQN is a unique identifier for one person, and BMXWT is the subject’s weight in kilograms (“BMX” is the NHANES prefix for body measurements). The variables in the NHANES data set are documented in a set of “codebooks” that are available on-line. The codebooks for the 2015-2016 wave of NHANES can be found by following the links at the following page:\nhttps://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2015\nFor convenience, direct links to some of the code books are included below:\n\nDemographics code book\nBody measures code book\nBlood pressure code book\nAlcohol questionaire code book\nSmoking questionaire code book\n\nEvery variable in a Pandas data frame has a data type. There are many different data types, but most commonly you will encounter floating point values (real numbers), integers, strings (text), and date/time values. When Pandas reads a text/csv file, it guesses the data types based on what it sees in the first few rows of the data file. Usually it selects an appropriate type, but occasionally it does not. To confirm that the data types are consistent with what the variables represent, inspect the ‘dtypes’ attribute of the data frame.\n\nda.dtypes\n\nSEQN          int64\nALQ101      float64\nALQ110      float64\nALQ130      float64\nSMQ020        int64\nRIAGENDR      int64\nRIDAGEYR      int64\nRIDRETH1      int64\nDMDCITZN    float64\nDMDEDUC2    float64\nDMDMARTL    float64\nDMDHHSIZ      int64\nWTINT2YR    float64\nSDMVPSU       int64\nSDMVSTRA      int64\nINDFMPIR    float64\nBPXSY1      float64\nBPXDI1      float64\nBPXSY2      float64\nBPXDI2      float64\nBMXWT       float64\nBMXHT       float64\nBMXBMI      float64\nBMXLEG      float64\nBMXARML     float64\nBMXARMC     float64\nBMXWAIST    float64\nHIQ210      float64\ndtype: object\n\n\nAs we see here, most of the variables have floating point or integer data type. Unlike many data sets, NHANES does not use any text values in its data. For example, while many datasets would use text labels like “F” or “M” to denote a subject’s gender, this information is represented in NHANES with integer codes. The actual meanings of these codes can be determined from the codebooks. For example, the variable RIAGENDR contains each subject’s gender, with male gender coded as 1 and female gender coded as 2. The RIAGENDR variable is part of the demographics component of NHANES, so this coding can be found in the demographics codebook.\nVariables like BMXWT which represent a quantitative measurement will typically be stored as floating point data values.\n\n\n6.0.4 Slicing a data set\nAs discussed above, a Pandas data frame is a rectangular data table, in which the rows represent cases and the columns represent variables. One common manipulation of a data frame is to extract the data for one case or for one variable. There are several ways to do this, as shown below.\nTo extract all the values for one variable, the following three approaches are equivalent (“DMDEDUC2” here is an NHANES variable containing a person’s educational attainment). In these four lines of code, we are assigning the data from one column of the data frame da into new variables w, x, y, and z. The first three approaches access the variable by name. The fourth approach accesses the variable by position (note that DMDEDUC2 is in position 9 of the da.columns array shown above – remember that Python counts starting at position zero).\n\nw = da[\"DMDEDUC2\"]\nx = da.loc[:, \"DMDEDUC2\"]\ny = da.DMDEDUC2\nz = da.iloc[:, 9]  # DMDEDUC2 is in column 9\n\nAnother reason to slice a variable out of a data frame is so that we can then pass it into a function. For example, we can find the maximum value over all DMDEDUC2 values using any one of the following four lines of code:\n\nprint(da[\"DMDEDUC2\"].max())\nprint(da.loc[:, \"DMDEDUC2\"].max())\nprint(da.DMDEDUC2.max())\nprint(da.iloc[:, 9].max())\n\n9.0\n9.0\n9.0\n9.0\n\n\nEvery value in a Python program has a type, and the type information can be obtained using Python’s ‘type’ function. This can be useful, for example, if you are looking for the documentation associated with some value, but you do not know what the value’s type is.\nHere we see that the variable da has type ‘DataFrame’, while one column of da has type ‘Series’. As noted above, a Series is a Pandas data structure for holding a single column (or row) of data.\n\nprint(type(da)) # The type of the variable\nprint(type(da.DMDEDUC2)) # The type of one column of the data frame\nprint(type(da.iloc[2,:])) # The type of one row of the data frame\n\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.series.Series'>\n\n\nIt may also be useful to slice a row (case) out of a data frame. Just as a data frame’s columns have names, the rows also have names, which are called the “index”. However many data sets do not have meaningful row names, so it is more common to extract a row of a data frame using its position. The iloc method slices rows or columns from a data frame by position (counting from 0). The following line of code extracts row 3 from the data set (which is the fourth row, counting from zero).\n\nx = da.iloc[3, :]\n\nAnother important data frame manipulation is to extract a contiguous block of rows or columns from the data set. Below we slice by position, in the first case taking row positions 3 and 4 (counting from 0, which are rows 4 and 5 counting from 1), and in the second case taking columns 2, 3, and 4 (columns 3, 4, 5 if counting from 1).\n\nx = da.iloc[3:5, :]\ny = da.iloc[:, 2:5]\n\n\n\n6.0.5 Missing values\nWhen reading a dataset using Pandas, there is a set of values including ‘NA’, ‘NULL’, and ‘NaN’ that are taken by default to represent a missing value. The full list of default missing value codes is in the ‘read_csv’ documentation here. This document also explains how to change the way that ‘read_csv’ decides whether a variable’s value is missing.\nPandas has functions called isnull and notnull that can be used to identify where the missing and non-missing values are located in a data frame. Below we use these functions to count the number of missing and non-missing DMDEDUC2 values.\n\nprint(pd.isnull(da.DMDEDUC2).sum())\nprint(pd.notnull(da.DMDEDUC2).sum())\n\n261\n5474\n\n\nAs an aside, note that there may be a variety of distinct forms of missingness in a variable, and in some cases it is important to keep these values distinct. For example, in case of the DMDEDUC2 variable, in addition to the blank or NA values that Pandas considers to be missing, three people responded “don’t know” (code value 9). In many analyses, the “don’t know” values will also be treated as missing, but at this point we are considering “don’t know” to be a distinct category of observed response."
  },
  {
    "objectID": "code/6_week1_python_resources.html",
    "href": "code/6_week1_python_resources.html",
    "title": "7  Python Resources",
    "section": "",
    "text": "7.0.1 The Python Documentation\nAny reference that does not begin with the Python documentation would not be complete. The authors of the language, as well as the community that supports it, have developed a great set of tutorials, documentation, and references around Python. When in doubt, this is often the first place that you should look if you run into a scary error or would like to learn more about a specific function. The documentation can be found here: Python Documentation\n\n\n7.0.2 Python Programming Introductions\nBelow are resources to help you along your way in learning Python. While it is great to consume material, in programming there is no substitute for actually writing code. For every hour that you spend learning, you should spend about twice that amount of time writing code for cool problems or working out examples. Coding is best learned through actually coding!\n\nCoursera has several offerings for Python that you can take in addition to this course. These courses will go into depth into Python programming and how to use it in an applied setting\nCode Academy is another resources that is great for learning Python (and other programming languages). While not as focused as Cousera, this is a quick way to get up-and-running with Python\nYouTube is another great resource for online learning and there are several “courses” for learning Python. We recommend trying several sets of videos to see which you like best and using multiple video series to learn since each will present the material in a slightly different way\nThere are tens of books on programming in Python that are great if you prefer to read. More so than the other resources, be sure to code what you learn. It is easy to read about coding, but you really learn to code by coding!\nIf you have a background in coding, the authors have found the tutorial at Tutorials Point to be useful in getting started with Python. This tutorial assumes that you have some background in coding in another language\n\n\n\n7.0.3 Cheatsheets and References\nThere are a variety of one-pagers and cheat-sheets available for Python that summarize the language in a few simple pages. These resources tend to be more aimed at someone who knows the language, or has experience in the language, but would like a refresher course in how the language works.\n\nCheatsheet for Numpy\nCheatsheet for Datawrangling\nCheatsheet for Pandas\nCheatsheet for SciPy\nCheatsheet for Matplotlib\n\n\n\n7.0.4 Python Style Guides\nAs you learn to code, you will find that you will begin to develop your own style. Sometimes this is good. Most times, this can be detrimental to your code readability and, worse, can hinder you from finding bugs in your own code in extreme cases.\nIt is best to learn good coding habits from the beginning and the Google Style Guide is a great place to start. We will mention some of these best practices here.\n\n7.0.4.1 Consistent Indenting\nPython will generally ‘yell’ at you if your indenting is incorrect. It is good to use an editor that takes care of this for you. In general, four spaces are preferred for indenting and you should not mix tabs and spaces.\n\n# Good Indenting - four spaces are standard but consistiency is key\nresult = []\nfor x in range(10):\n    for y in range(5):\n        if x * y > 10:\n            result.append((x, y))\nprint (result)\n\n# Bad indenting\nresult = []\nfor x in range(10):\n  for y in range(5):\n     if x * y > 10:\n               result.append((x, y))\nprint (result)\n\n\n\n7.0.4.2 Commenting\nComments seem weird when you first begin programming - why would I include ‘code’ that doesn’t run? Comments are probably some of the most important aspects of code. They help other read code that is difficult for them to understand, and they, more importantly, are helpful for yourself if you look at the code in a few weeks and need clarity on why you did something. Always comment and comment well.\n\n################################################################################\n#                                                                              #\n#                               Good Commenting                                #\n#                                                                              #\n################################################################################\n\n################################ Bad Commenting ################################\n\n# My loop\nfor x in range(10):\n    print (x)\n    \n############################## Better Commenting ###############################\n\n# Looping from zero to ten\nfor x in range(10):\n    print (x)\n\n############################# Preferred Commenting #############################\n\n# Print out the numbers from zero to ten\nfor x in range(10):\n    print (x)\n\n\n################################################################################\n#                                                                              #\n#                         Mixing Commenting Strategies                         #\n#                                                                              #\n################################################################################\n\n# Try not to mix commenting styles in the same blocks - just be consistient\n\n########### Bad - mixing doc-strings commenting and line commenting ############\n\n''' Printing one to five, a six, and then six to nine'''\nfor x in range(10):\n    # If x > 5, then print the value\n    if x > 5: \n        print (x)\n    else:\n        print (x + 1)\n        \n##################### Good - no mixing of comment types ########################\n\n# Printing one to five, a six, and then six to nine\nfor x in range(10):\n    # If x > 5, then print the value\n    if x > 5: \n        print (x)\n    else:\n        print (x + 1)\n\n\n\n7.0.4.3 Line Length\nTry to avoid excessively long lines. Standard practice is to keep lines to no longer than 80 characters. While this is not a hard rule, it is a good practice to follow for readability\n\n######################### Bad - This code is too long ##########################\n\nmy_random_array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n############ Good - this code is wrapped to avoid excessive length #############\n\nmy_random_array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n                   10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, \n                   9, 10]\n\n\n\n7.0.4.4 White Space\nUtilizing Whitespace is a great way to improve the way that your code looks. In general the following can be helpful to improve the look of your code\n\nTry to space out your code and introduce whitespace to improve readability\nUse spacing to separate function arguments\nDo not over-do spacing. Too many spaces between code blocks makes it difficult to organize code well\n\n\n################ Bad - this code has bad whitespace management #################\n\nmy_player = player()\nplayer_attributes = get_player_attributes(my_player,height,weight,   birthday)\n\n\nplayer_attributes[0]*=12 # convert from feet to inches\n\n\n\n\nplayer.shoot_ball()\n\n\n########################## Good whitespace management ##########################\n\nmy_player = player()\nplayer_attributes = get_player_attributes(my_player, height, weight, birthday)\n\n# convert from feet to inches\nplayer_attributes[0] *= 12 \n\nplayer.shoot_ball()\n\n\n\n7.0.4.5 The tip of the iceberg\nTake a look at code out in the wild if you are really curious. How are they coding specific things? How do they manage spacing in loops? How do they manage the whitespace in argument list?\nYou will learn to code by coding, and you will develop your own style but starting out with good habits ensures that your code is easy to read by others and, most importantly, yourself. Good luck!"
  },
  {
    "objectID": "code/7_python_libraries.html",
    "href": "code/7_python_libraries.html",
    "title": "8  Python Libraries",
    "section": "",
    "text": "For this tutorial, we are going to outline the most common uses for each of the following libraries:\nImportant: While this tutorial provides insight into the basics of these libraries, I recommend digging into the documentation that is available online."
  },
  {
    "objectID": "code/7_python_libraries.html#numpy",
    "href": "code/7_python_libraries.html#numpy",
    "title": "8  Python Libraries",
    "section": "8.1 NumPy",
    "text": "8.1 NumPy\nNumPy is the fundamental package for scientific computing with Python. It contains among other things:\n\na powerful N-dimensional array object\nsophisticated (broadcasting) functions\ntools for integrating C/C++ and Fortran code\nuseful linear algebra, Fourier transform, and random number capabilities\n\nWe will focus on the numpy array object.\n\n8.1.1 Numpy Array\nA numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\n\nimport numpy as np\n\n\n### Create a 3x1 numpy array\na = np.array([1,2,3])\nprint(a)\n\n### Print object type\nprint(type(a))\n\n[1 2 3]\n<class 'numpy.ndarray'>\n\n\n\n### Print shape\n#we have a one dimensional object with 3 values\nprint(a.shape)\n\n(3,)\n\n\n\n### Print some values in a\nprint(a[0], a[1], a[2])\n\n1 2 3\n\n\n\n### Create a 2x2 numpy array using a nested list\nb = np.array([[1,2],[3,4]])\nprint(b)\n\n[[1 2]\n [3 4]]\n\n\n\n### Print shape\n# we now have a two dimensional array (with 2 rows and 2 columns)\nprint(b.shape)\n\n(2, 2)\n\n\n\n#in row two, access the the first value\n#here we index, specifying the row first and then the column position\n#don't forget: in py we start counting at 0\nprint(b[1,0])\n\n3\n\n\n\n## Print several values in b\nprint(b[0,0], b[0,1], b[1,1])\n\n1 2 4\n\n\n\n### Create a 3x2 numpy array\nc = np.array([[1,2],[3,4],[5,6]])\nc\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\n### Print shape\nprint(c.shape)\n\n(3, 2)\n2 3 5 6\n\n\n\n### Print some values in c\nprint(c[0,1], c[1,0], c[2,0], c[2,1])\n\n2 3 5 6\n\n\n\n8.1.1.1 Create numpy arrays with different automatic numberings\n\n### create a 2x3 zero array with only 0s\nd = np.zeros((2,3))\n\nprint(d)\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n\n\n\n### 4x2 array of ones\ne = np.ones((4,2))\n\nprint(e)\n\n[[1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]]\n\n\n\n### create 2x2 constant array with a specified value\n#we first give the nr of rows and columns we want, followed by the constant value\nf = np.full((2,2), 9)\n\nprint(f)\n\n[[9 9]\n [9 9]]\n\n\n\n### create a 3x3 random array with random nrs\ng = np.random.random((3,3))\n\nprint(g)\n\n[[0.75578673 0.52378499 0.68715926]\n [0.09153656 0.89729222 0.85334664]\n [0.34651959 0.06094491 0.15857276]]\n\n\n\n\n\n8.1.2 Array Indexing\n\n### Create 3x4 array\nh = np.array([[1,2,3,4,], [5,6,7,8], [9,10,11,12]])\n\nprint(h)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\nprint(h[0,1])\n\n2\n\n\n\n### Slice array to make a 2x2 sub-array\n#first we select rows with index 0 and 1 (so up to but not including 2)\n#then we further select columns with index 1 and 2\ni = h[:2, 1:3]\n\nprint(i)\n\n[[2 3]\n [6 7]]\n\n\n\n### Modify something in the slice\ni[0,0] = 1738\nprint(i)\n\n[[1738    3]\n [   6    7]]\n\n\n\n#notice how this value is also changed in our original array h!\nprint(h)\n\n[[   1 1738    3    4]\n [   5    6    7    8]\n [   9   10   11   12]]\n\n\n\n\n8.1.3 Datatypes in Arrays\n\n### Integer\nj = np.array([1, 2])\nprint(j)\nprint(j.dtype)  \n\n[1 2]\nint64\n\n\n\n### Float\nk = np.array([1.2, 2.0])\nprint(k)\nprint(k.dtype)         \n\n[1.2 2. ]\nfloat64\n\n\n\n### Force Data Type\nl = np.array([1.0, 2.0], dtype = np.int64)\nprint(l)\nprint(l.dtype)\n\n[1 2]\nint64\n\n\n\n\n8.1.4 Array Math\nBasic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module:\n\nx = np.array([[1,2],[3,4]], dtype = np.float64)\ny = np.array([[5,6],[7,8]], dtype = np.float64)\nprint(x)\n\n[[1. 2.]\n [3. 4.]]\n\n\n\nprint(y)\n\n[[5. 6.]\n [7. 8.]]\n\n\n\n# Elementwise sum; both produce the array\n# [[ 6.0  8.0]\n#  [10.0 12.0]]\nprint(x + y)\n\n[[ 6.  8.]\n [10. 12.]]\n\n\n\nprint(np.add(x, y))\n\n[[ 6.  8.]\n [10. 12.]]\n\n\n\n# Elementwise difference; both produce the array\n# [[-4.0 -4.0]\n#  [-4.0 -4.0]]\nprint(x - y)\n\n[[-4. -4.]\n [-4. -4.]]\n\n\n\nprint(np.subtract(x, y))\n\n[[-4. -4.]\n [-4. -4.]]\n\n\n\n# Elementwise product; both produce the array\n# [[ 5.0 12.0]\n#  [21.0 32.0]]\nprint(x * y)\n\n[[ 5. 12.]\n [21. 32.]]\n\n\n\nprint(np.multiply(x, y))\n\n[[ 5. 12.]\n [21. 32.]]\n\n\n\n# Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\n\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n\n\n\nprint(np.divide(x, y))\n\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n\n\n\n# Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n\n[[1.         1.41421356]\n [1.73205081 2.        ]]\n\n\n\n\n8.1.5 Descriptive statistics with numpy\n\nx = np.array([[1,2],[3,4]])\nx \n\narray([[1, 2],\n       [3, 4]])\n\n\n\n### Compute sum of all elements; prints \"10\"\nprint(np.sum(x))\n\n10\n\n\n\n### Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=0)) \n\n[4 6]\n\n\n\n### Compute sum of each row; prints \"[3 7]\"\nprint(np.sum(x, axis=1))\n\n[3 7]\n\n\n\n### Compute mean of all elements; prints \"2.5\"\nprint(np.mean(x))\n\n2.5\n\n\n\n### Compute mean of each column; prints \"[2 3]\"\nprint(np.mean(x, axis=0)) \n\n[2. 3.]\n\n\n\n### Compute mean of each row; prints \"[1.5 3.5]\"\nprint(np.mean(x, axis=1))\n\n[1.5 3.5]"
  },
  {
    "objectID": "code/7_python_libraries.html#scipy",
    "href": "code/7_python_libraries.html#scipy",
    "title": "8  Python Libraries",
    "section": "8.2 SciPy",
    "text": "8.2 SciPy\nNumpy provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays. SciPy builds on this, and provides a large number of functions that operate on numpy arrays and are useful for different types of scientific and engineering applications.\nFor this course, we will primariyl be using the SciPy.Stats sub-library.\n\n8.2.1 SciPy.Stats\nThe SciPy.Stats module contains a large number of probability distributions as well as a growing library of statistical functions such as:\n\nContinuous and Discrete Distributions (i.e Normal, Uniform, Binomial, etc.)\nDescriptive Statistcs\nStatistical Tests (i.e T-Test)\n\n\nfrom scipy import stats\nimport numpy as np\n\n\n### Print 10 Normal Random Variables\nprint(stats.norm.rvs(size = 10))\n\n[ 1.2273344  -1.18292396  0.06328786 -1.08124849 -0.6143745   0.703326\n  1.11746254  1.06465073  0.64391558 -1.8944723 ]\n\n\n\nfrom pylab import *\n\n# Create some test data\ndx = .01\nX  = np.arange(-2,2,dx)\nY  = exp(-X**2)\n\n#print(X)\n#print(Y)\n\n\n# Normalize the data to a proper PDF\nY /= (dx*Y).sum()\n\n# Compute the CDF\nCY = np.cumsum(Y*dx)\n\n# Plot both\nplot(X,Y)\nplot(X,CY,'r--')\n\nshow()\n\n\n\n\n\n### Compute the Normal CDF of certain values.\n#this returns some probabilites based on the plot above\nprint(stats.norm.cdf(np.array([1,-1., 0, 1, 3, 4, -2, 6])))\n\n[0.84134475 0.15865525 0.5        0.84134475 0.9986501  0.99996833\n 0.02275013 1.        ]\n\n\n\n8.2.1.1 Descriptive Statistics\n\nnp.random.seed(282629734)\n\n# Generate 1000 Student’s T continuous random variables.\nx = stats.t.rvs(10, size=1000)\n\n\n# Do some descriptive statistics\nprint(x.min())   # equivalent to np.min(x)\n\n-3.7897557242248197\n\n\n\nprint(x.max())   # equivalent to np.max(x)\n\n5.263277329807165\n\n\n\nprint(x.mean())  # equivalent to np.mean(x)\n\n0.014061066398468422\n\n\n\nprint(x.var())   # equivalent to np.var(x))\n\n1.288993862079285\n\n\n\nstats.describe(x)\n\nDescribeResult(nobs=1000, minmax=(-3.7897557242248197, 5.263277329807165), mean=0.014061066398468422, variance=1.2902841462255106, skewness=0.21652778283120955, kurtosis=1.055594041706331)\n\n\nLater in the course, we will discuss distributions and statistical tests such as a T-Test. SciPy has built in functions for these operations."
  },
  {
    "objectID": "code/7_python_libraries.html#matplotlib",
    "href": "code/7_python_libraries.html#matplotlib",
    "title": "8  Python Libraries",
    "section": "8.3 MatPlotLib",
    "text": "8.3 MatPlotLib\nMatplotlib is a plotting library. In this section give a brief introduction to the matplotlib.pyplot module.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()  # You must call plt.show() to make graphics appear.\n\n\n\n\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\nplt.show()\n\n\n\n\n\n8.3.0.1 Subplots\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()"
  },
  {
    "objectID": "code/7_python_libraries.html#seaborn",
    "href": "code/7_python_libraries.html#seaborn",
    "title": "8  Python Libraries",
    "section": "8.4 Seaborn",
    "text": "8.4 Seaborn\nSeaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and makes generating visualizations convenient.\nWhile Matplotlib is a robust solution for various problems, Seaborn utilizes more concise paramesters for ease-of-use.\n\n8.4.0.1 Scatterplots\n\n# Import necessary libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Store the url string that hosts our .csv file\nurl = \"../data/Cartwheeldata.csv\"\n\n# Read the .csv file and store it as a pandas Data Frame\ndf = pd.read_csv(url)\n\n# Create Scatterplot\nsns.lmplot(x='Wingspan', y='CWDistance', data=df)\n\nplt.show()\n\n\n\n\n\n# Scatterplot arguments\nsns.lmplot(x='Wingspan', y='CWDistance', data=df,\n           fit_reg=False, # No regression line\n           hue='Gender')   # Color by evolution stage\n\nplt.show()\n\n\n\n\n\n# Construct Cartwheel distance plot\nsns.swarmplot(x=\"Gender\", y=\"CWDistance\", data=df)\n\nplt.show()\n\n\n\n\n\n\n8.4.0.2 Boxplots\n\nsns.boxplot(data=df.loc[:, [\"Age\", \"Height\", \"Wingspan\", \"CWDistance\", \"Score\"]])\n\nplt.show()\n\n\n\n\n\n# Male Boxplot\nsns.boxplot(data=df.loc[df['Gender'] == 'M', [\"Age\", \"Height\", \"Wingspan\", \"CWDistance\", \"Score\"]])\n\nplt.show()\n\n\n\n\n\n# Female Boxplot\nsns.boxplot(data=df.loc[df['Gender'] == 'F', [\"Age\", \"Height\", \"Wingspan\", \"CWDistance\", \"Score\"]])\n\nplt.show()\n\n\n\n\n\n# Male Boxplot\nsns.boxplot(data=df.loc[df['Gender'] == 'M', [\"Score\"]])\n\nplt.show()\n\n\n\n\n\n# Female Boxplot\nsns.boxplot(data=df.loc[df['Gender'] == 'F', [\"Score\"]])\n\nplt.show()\n\n\n\n8.4.0.3 Histogram\n\n# Distribution Plot (a.k.a. Histogram)\nsns.displot(df.CWDistance)\n\nplt.show()\n\n\n\n\n\n\n8.4.0.4 Count Plot\n\n# Count Plot (a.k.a. Bar Plot)\nsns.countplot(x='Gender', data=df)\n \nplt.xticks(rotation=-45)\n\nplt.show()"
  },
  {
    "objectID": "code/8_Tables_Histograms_and_Boxplots_in_Python.html",
    "href": "code/8_Tables_Histograms_and_Boxplots_in_Python.html",
    "title": "9  Visualizing Data in Python",
    "section": "",
    "text": "9.0.0.2 Visualizing the Data - Tables\nWhen you begin working with a new data set, it is often best to print out the first few rows before you begin other analysis. This will show you what kind of data is in the dataset, what data types you are working with, and will serve as a reference for the other plots that we are about to make.\n\n# Print out the first few rows of the data\ntips_data.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n\n\n\n\n9.0.0.3 Describing Data\nSummary statistics, which include things like the mean, min, and max of the data, can be useful to get a feel for how large some of the variables are and what variables may be the most important.\n\n# Print out the summary statistics for the quantitative variables\ntips_data.describe()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      size\n    \n  \n  \n    \n      count\n      244.000000\n      244.000000\n      244.000000\n    \n    \n      mean\n      19.785943\n      2.998279\n      2.569672\n    \n    \n      std\n      8.902412\n      1.383638\n      0.951100\n    \n    \n      min\n      3.070000\n      1.000000\n      1.000000\n    \n    \n      25%\n      13.347500\n      2.000000\n      2.000000\n    \n    \n      50%\n      17.795000\n      2.900000\n      2.000000\n    \n    \n      75%\n      24.127500\n      3.562500\n      3.000000\n    \n    \n      max\n      50.810000\n      10.000000\n      6.000000\n    \n  \n\n\n\n\n\n\n9.0.0.4 Creating a Histogram\nAfter we have a general ‘feel’ for the data, it is often good to get a feel for the shape of the distribution of the data.\n\n# Plot a histogram of the total bill\n#kde --> whether or not to display a density plot\nplot = sns.displot(tips_data[\"total_bill\"], kde = False)\nplt.title(\"Histogram of Total Bill\")\nplt.show()\n\n\n\n\n\n# Plot a histogram of the Tips only\nsns.displot(tips_data[\"tip\"], kde = True)\nplt.title(\"Histogram of Total Tip\")\nplt.show()\n\n\n\n\n\n# Plot a histogram of both the total bill and the tips'\nsns.histplot(tips_data[\"total_bill\"], kde = False)\nsns.histplot(tips_data[\"tip\"], kde = False)\nplt.show()\n\n\n\n\n\n#alternative\nfig, ax =plt.subplots(1,2)\nsns.histplot(tips_data[\"total_bill\"], kde = False, ax = ax[0])\nsns.histplot(tips_data[\"tip\"], kde = False, ax = ax[1])\nplt.show()\n\n\n\n\n\n\n9.0.0.5 Creating a Boxplot\nBoxplots do not show the shape of the distribution, but they can give us a better idea about the center and spread of the distribution as well as any potential outliers that may exist. Boxplots and Histograms often complement each other and help an analyst get more information about the data\n\n# Create a boxplot of the total bill amounts\nsns.boxplot(tips_data[\"total_bill\"])\nplt.title(\"Box plot of the Total Bill\")\n\nplt.show()\n\n\n\n\n\n# Create a boxplot of the tips amounts\nsns.boxplot(tips_data[\"tip\"])\nplt.title(\"Box plot of the Tip\")\n\nplt.show()\n\n\n\n\n\n# Create a boxplot of the tips and total bill amounts - do not do it like this\nsns.boxplot(tips_data[\"total_bill\"])\nplt.title(\"Box plot of the Total Bill and Tips\")\nsns.boxplot(tips_data[\"tip\"])\n\nplt.show()\n\n\n\n\n\n\n9.0.0.6 Creating Histograms and Boxplots Plotted by Groups\nWhile looking at a single variable is interesting, it is often useful to see how a variable changes in response to another. Using graphs, we can see if there is a difference between the tipping amounts of smokers vs. non-smokers, if tipping varies according to the time of the day, or we can explore other trends in the data as well.\n\n# Create a boxplot and histogram of the tips grouped by smoking status\n# x = what I am trying to plot\n# y = what I am going to be grouping by\nsns.boxplot(x = tips_data[\"tip\"], y = tips_data[\"smoker\"])\nplt.show()\n\n\n\n\n\n# Create histograms of the tips grouped by smoking status\n\n#set up a facet grid by saying we want to have two similar boxes for our two smoking categories\ng = sns.FacetGrid(tips_data, row = \"smoker\")\n\n#the map fct allows us to take the histrogram feature of plt and map it across both smoking groups at the same time\ng = g.map(plt.hist, \"tip\")\n\nplt.show()\n\n\n\n\n\n# Create a boxplot and histogram of the tips grouped by time of day\nsns.boxplot(x = tips_data[\"tip\"], y = tips_data[\"time\"])\n\ng = sns.FacetGrid(tips_data, row = \"time\")\ng = g.map(plt.hist, \"tip\")\nplt.show()\n\n\n\n\n\n\n\n\n# Create a boxplot and histogram of the tips grouped by the day\nsns.boxplot(x = tips_data[\"tip\"], y = tips_data[\"day\"])\n\ng = sns.FacetGrid(tips_data, row = \"day\")\ng = g.map(plt.hist, \"tip\")\nplt.show()"
  },
  {
    "objectID": "code/9_nhanes_univariate_analyses.html",
    "href": "code/9_nhanes_univariate_analyses.html",
    "title": "10  Univariate data analyses - NHANES case study",
    "section": "",
    "text": "The following import statements make the libraries that we will need available. Note that in a Jupyter notebook, you should generally use the %matplotlib inline directive, which would not be used when running a script outside of the Jupyter environment.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nNext we will load the NHANES data from a file.\n\nda = pd.read_csv(\"../data/nhanes_2015_2016.csv\")\nda.head()\n\n\n\n\n\n  \n    \n      \n      SEQN\n      ALQ101\n      ALQ110\n      ALQ130\n      SMQ020\n      RIAGENDR\n      RIDAGEYR\n      RIDRETH1\n      DMDCITZN\n      DMDEDUC2\n      ...\n      BPXSY2\n      BPXDI2\n      BMXWT\n      BMXHT\n      BMXBMI\n      BMXLEG\n      BMXARML\n      BMXARMC\n      BMXWAIST\n      HIQ210\n    \n  \n  \n    \n      0\n      83732\n      1.0\n      NaN\n      1.0\n      1\n      1\n      62\n      3\n      1.0\n      5.0\n      ...\n      124.0\n      64.0\n      94.8\n      184.5\n      27.8\n      43.3\n      43.6\n      35.9\n      101.1\n      2.0\n    \n    \n      1\n      83733\n      1.0\n      NaN\n      6.0\n      1\n      1\n      53\n      3\n      2.0\n      3.0\n      ...\n      140.0\n      88.0\n      90.4\n      171.4\n      30.8\n      38.0\n      40.0\n      33.2\n      107.9\n      NaN\n    \n    \n      2\n      83734\n      1.0\n      NaN\n      NaN\n      1\n      1\n      78\n      3\n      1.0\n      3.0\n      ...\n      132.0\n      44.0\n      83.4\n      170.1\n      28.8\n      35.6\n      37.0\n      31.0\n      116.5\n      2.0\n    \n    \n      3\n      83735\n      2.0\n      1.0\n      1.0\n      2\n      2\n      56\n      3\n      1.0\n      5.0\n      ...\n      134.0\n      68.0\n      109.8\n      160.9\n      42.4\n      38.5\n      37.7\n      38.3\n      110.1\n      2.0\n    \n    \n      4\n      83736\n      2.0\n      1.0\n      1.0\n      2\n      2\n      42\n      4\n      1.0\n      4.0\n      ...\n      114.0\n      54.0\n      55.2\n      164.9\n      20.3\n      37.4\n      36.0\n      27.2\n      80.4\n      2.0\n    \n  \n\n5 rows × 28 columns\n\n\n\n\n10.0.1 Frequency tables\nThe value_counts method can be used to determine the number of times that each distinct value of a variable occurs in a data set. In statistical terms, this is the “frequency distribution” of the variable. Below we show the frequency distribution of the DMDEDUC2 variable, which is a variable that reflects a person’s level of educational attainment. The value_counts method produces a table with two columns. The first column contains all distinct observed values for the variable. The second column contains the number of times each of these values occurs. Note that the table returned by value_counts is actually a Pandas data frame, so can be further processed using any Pandas methods for working with data frames.\nThe numbers 1, 2, 3, 4, 5, 9 seen below are integer codes for the 6 possible non-missing values of the DMDEDUC2 variable. The meaning of these codes is given in the NHANES codebook located here, and will be discussed further below. This table shows, for example, that 1621 people in the data file have DMDEDUC=4, which indicates that the person has completed some college, but has not graduated with a four-year degree.\n\nda.DMDEDUC2.value_counts()\n\n4.0    1621\n5.0    1366\n3.0    1186\n1.0     655\n2.0     643\n9.0       3\nName: DMDEDUC2, dtype: int64\n\n\nNote that the value_counts method excludes missing values. We confirm this below by adding up the number of observations with a DMDEDUC2 value equal to 1, 2, 3, 4, 5, or 9 (there are 5474 such rows), and comparing this to the total number of rows in the data set, which is 5735. This tells us that there are 5735 - 5474 = 261 missing values for this variable (other variables may have different numbers of missing values).\n\nprint(da.DMDEDUC2.value_counts().sum())\nprint(1621 + 1366 + 1186 + 655 + 643 + 3) # Manually sum the frequencies\nprint(da.shape)\n\n5474\n5474\n(5735, 28)\n\n\nAnother way to obtain this result is to locate all the null (missing) values in the data set using the isnull Pandas function, and count the number of such locations.\n\npd.isnull(da.DMDEDUC2).sum()\n\n261\n\n\n\n10.0.1.1 Replace naming in a column\nIn some cases it is useful to replace integer codes with a text label that reflects the code’s meaning. Below we create a new variable called ‘DMDEDUC2x’ that is recoded with text labels, then we generate its frequency distribution.\n\nda[\"DMDEDUC2x\"] = da.DMDEDUC2.replace({1: \"<9\", 2: \"9-11\", 3: \"HS/GED\", 4: \"Some college/AA\", 5: \"College\", \n                                       7: \"Refused\", 9: \"Don't know\"})\n\nda.DMDEDUC2x.value_counts()\n\nSome college/AA    1621\nCollege            1366\nHS/GED             1186\n<9                  655\n9-11                643\nDon't know            3\nName: DMDEDUC2x, dtype: int64\n\n\nWe will also want to have a relabeled version of the gender variable, so we will construct that now as well. We will follow a convention here of appending an ‘x’ to the end of a categorical variable’s name when it has been recoded from numeric to string (text) values.\n\nda[\"RIAGENDRx\"] = da.RIAGENDR.replace({1: \"Male\", 2: \"Female\"})\n\nda[\"RIAGENDRx\"].value_counts()\n\nFemale    2976\nMale      2759\nName: RIAGENDRx, dtype: int64\n\n\nFor many purposes it is more relevant to consider the proportion of the sample with each of the possible category values, rather than the number of people in each category. We can do this as follows:\n\nx = da.DMDEDUC2x.value_counts()  # x is just a name to hold this value temporarily\nx / x.sum() * 100\n\nSome college/AA    29.612715\nCollege            24.954330\nHS/GED             21.666058\n<9                 11.965656\n9-11               11.746438\nDon't know          0.054805\nName: DMDEDUC2x, dtype: float64\n\n\n\n\n10.0.1.2 Replace NAs with another category\nIn some cases we will want to treat the missing response category as another category of observed response, rather than ignoring it when creating summaries. Below we create a new category called “Missing”, and assign all missing values to it usig fillna. Then we recalculate the frequency distribution. We see that 4.6% of the responses are missing.\n\nda[\"DMDEDUC2x\"] = da.DMDEDUC2x.fillna(\"Missing\")\nx = da.DMDEDUC2x.value_counts()\nx / x.sum() * 100\n\nSome college/AA    28.265039\nCollege            23.818657\nHS/GED             20.680035\n<9                 11.421099\n9-11               11.211857\nMissing             4.551003\nDon't know          0.052310\nName: DMDEDUC2x, dtype: float64\n\n\n\n\n\n10.0.2 Numerical summaries\nA quick way to get a set of numerical summaries for a quantitative variable is with the describe data frame method. Below we demonstrate how to do this using the body weight variable (BMXWT). As with many surveys, some data values are missing, so we explicitly drop the missing cases using the dropna method before generating the summaries.\n\nda.BMXWT.dropna().describe()\n\ncount    5666.000000\nmean       81.342676\nstd        21.764409\nmin        32.400000\n25%        65.900000\n50%        78.200000\n75%        92.700000\nmax       198.900000\nName: BMXWT, dtype: float64\n\n\nIt’s also possible to calculate individual summary statistics from one column of a data set. This can be done using Pandas methods, or with numpy functions:\n\nx = da.BMXWT.dropna()  # Extract all non-missing values of BMXWT into a variable called 'x'\nprint(x.mean()) # Pandas method\nprint(np.mean(x)) # Numpy function\n\nprint(x.median())\nprint(np.percentile(x, 50))  # 50th percentile, same as the median\nprint(np.percentile(x, 75))  # 75th percentile\nprint(x.quantile(0.75)) # Pandas method for quantiles, equivalent to 75th percentile\n\n81.34267560889516\n81.34267560889516\n78.2\n78.2\n92.7\n92.7\n\n\nNext we look at frequencies for a systolic blood pressure measurement (BPXSY1). “BPX” here is the NHANES prefix for blood pressure measurements, “SY” stands for “systolic” blood pressure (blood pressure at the peak of a heartbeat cycle), and “1” indicates that this is the first of three systolic blood presure measurements taken on a subject.\nA person is generally considered to have pre-hypertension when their systolic blood pressure is between 120 and 139, or their diastolic blood pressure is between 80 and 89. Considering only the systolic condition, we can calculate the proprotion of the NHANES sample who would be considered to have pre-hypertension.\n\nnp.mean((da.BPXSY1 >= 120) & (da.BPXSY2 <= 139))  # \"&\" means \"and\"\n\n0.3741935483870968\n\n\nNext we calculate the propotion of NHANES subjects who are pre-hypertensive based on diastolic blood pressure.\n\nnp.mean((da.BPXDI1 >= 80) & (da.BPXDI2 <= 89))\n\n0.14803836094158676\n\n\nFinally we calculate the proportion of NHANES subjects who are pre-hypertensive based on either systolic or diastolic blood pressure. Since some people are pre-hypertensive under both criteria, the proportion below is less than the sum of the two proportions calculated above.\nSince the combined systolic and diastolic condition for pre-hypertension is somewhat complex, below we construct temporary variables ‘a’ and ‘b’ that hold the systolic and diastolic pre-hypertensive status separately, then combine them with a “logical or” to obtain the final status for each subject.\n\na = (da.BPXSY1 >= 120) & (da.BPXSY2 <= 139)\nb = (da.BPXDI1 >= 80) & (da.BPXDI2 <= 89)\nprint(np.mean(a | b))  # \"|\" means \"or\"\n\n0.43975588491717527\n\n\nBlood pressure measurements are affected by a phenomenon called “white coat anxiety”, in which a subject’s bood pressure may be slightly elevated if they are nervous when interacting with health care providers. Typically this effect subsides if the blood pressure is measured several times in sequence. In NHANES, both systolic and diastolic blood pressure are meausred three times for each subject (e.g. BPXSY2 is the second measurement of systolic blood pressure). We can calculate the extent to which white coat anxiety is present in the NHANES data by looking a the mean difference between the first two systolic or diastolic blood pressure measurements.\n\nprint(np.mean(da.BPXSY1 - da.BPXSY2))\nprint(np.mean(da.BPXDI1 - da.BPXDI2))\n\n0.6749860309182343\n0.3490407897187558\n\n\n\n\n10.0.3 Graphical summaries\nQuantitative variables can be effectively summarized graphically. Below we see the distribution of body weight (in Kg), shown as a histogram. It is evidently right-skewed.\n\nsns.histplot(da.BMXWT.dropna())\n\n<AxesSubplot:xlabel='BMXWT', ylabel='Count'>\n\n\n\n\n\nNext we look at the histogram of systolic blood pressure measurements. You can see that there is a tendency for the measurements to be rounded to the nearest 5 or 10 units.\n\nsns.histplot(da.BPXSY1.dropna())\n\n<AxesSubplot:xlabel='BPXSY1', ylabel='Count'>\n\n\n\n\n\nTo compare several distributions, we can use side-by-side boxplots. Below we compare the distributions of the first and second systolic blood pressure measurements (BPXSY1, BPXSY2), and the first and second diastolic blood pressure measurements (BPXDI1, BPXDI2). As expected, diastolic measurements are substantially lower than systolic measurements. Above we saw that the second blood pressure reading on a subject tended on average to be slightly lower than the first measurement. This difference was less than 1 mm/Hg, so is not visible in the “marginal” distributions shown below.\n\nbp = sns.boxplot(data=da[[\"BPXSY1\", \"BPXSY2\", \"BPXDI1\", \"BPXDI2\"]])\n_ = bp.set_ylabel(\"Blood pressure in mm/Hg\")\n\n\n\n\n\n\n10.0.4 Stratification\nOne of the most effective ways to get more information out of a dataset is to divide it into smaller, more uniform subsets, and analyze each of these “strata” on its own. We can then formally or informally compare the findings in the different strata. When working with human subjects, it is very common to stratify on demographic factors such as age, sex, and race.\nTo illustrate this technique, consider blood pressure, which is a value that tends to increase with age. To see this trend in the NHANES data, we can partition the data into age strata, and construct side-by-side boxplots of the systolic blood pressure (SBP) distribution within each stratum. Since age is a quantitative variable, we need to create a series of “bins” of similar SBP values in order to stratify the data. Each box in the figure below is a summary of univariate data within a specific population stratum (here defined by age).\n\nda.RIDAGEYR.value_counts()\n\n80    343\n18    133\n19    128\n60    119\n61    112\n     ... \n74     52\n78     47\n76     44\n77     43\n79     35\nName: RIDAGEYR, Length: 63, dtype: int64\n\n\n\nda[\"agegrp\"] = pd.cut(da.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80]) # Create age strata based on these cut points\n\nplt.figure(figsize=(12, 5))  # Make the figure wider than default (12cm wide by 5cm tall)\nsns.boxplot(x=\"agegrp\", y=\"BPXSY1\", data=da)  # Make boxplot of BPXSY1 stratified by age group\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8388799ef0>\n\n\n\n\n\nTaking this a step further, it is also the case that blood pressure tends to differ between women and men. While we could simply make two side-by-side boxplots to illustrate this contrast, it would be a bit odd to ignore age after already having established that it is strongly associated with blood pressure. Therefore, we will doubly stratify the data by gender and age.\nWe see from the figure below that within each gender, older people tend to have higher blood pressure than younger people. However within an age band, the relationship between gender and systolic blood pressure is somewhat complex – in younger people, men have substantially higher blood pressures than women of the same age. However for people older than 50, this relationship becomes much weaker, and among people older than 70 it appears to reverse. It is also notable that the variation of these distributions, reflected in the height of each box in the boxplot, increases with age.\n\nda[\"agegrp\"] = pd.cut(da.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80])\n\nplt.figure(figsize=(12, 5))\nsns.boxplot(x=\"agegrp\", y=\"BPXSY1\", hue=\"RIAGENDRx\", data = da)\n\n<AxesSubplot:xlabel='agegrp', ylabel='BPXSY1'>\n\n\n\n\n\nWhen stratifying on two factors (here age and gender), we can group the boxes first by age, and within age bands by gender, as above, or we can do the opposite – group first by gender, and then within gender group by age bands. Each approach highlights a different aspect of the data.\n\nda[\"agegrp\"] = pd.cut(da.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80])\n\nplt.figure(figsize=(12, 5))\nsns.boxplot(x=\"RIAGENDRx\", y=\"BPXSY1\", hue=\"agegrp\", data=da)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f838880ed68>\n\n\n\n\n\nStratification can also be useful when working with categorical variables. Below we look at the frequency distribution of educational attainment (“DMDEDUC2”) within 10-year age bands. While “some college” is the most common response in all age bands, up to around age 60 the second most common response is “college” (i.e. the person graduated from college with a four-year degree). However for people over 50, there are as many or more people with only high school or general equivalency diplomas (HS/GED) than there are college graduates.\nNote on causality and confounding: An important role of statistics is to aid researchers in identifying causes underlying observed differences. Here we have seen differences in both blood pressure and educational attainment based on age. It is plausible that aging directly causes blood pressure to increase. But in the case of educational attainment, this is actually a “birth cohort effect”. NHANES is a cross sectional survey (all data for one wave were collected at a single point in time). People who were, say, 65 in 2015 (when these data were collected), were college-aged around 1970, while people who were in their 20’s in 2015 were college-aged in around 2010 or later. Over the last few decades, it has become much more common for people to at least begin a college degree than it was in the past. Therefore, younger people as a group have higher educational attainment than older people as a group. As these young people grow older, the cross sectional relationship between age and educational attainment will change.\n\nda.groupby(\"agegrp\")[\"DMDEDUC2x\"].value_counts()\n\nagegrp    DMDEDUC2x      \n(18, 30]  Some college/AA    364\n          College            278\n          HS/GED             237\n          Missing            128\n          9-11                99\n          <9                  47\n(30, 40]  Some college/AA    282\n          College            264\n          HS/GED             182\n          9-11               111\n          <9                  93\n(40, 50]  Some college/AA    262\n          College            260\n          HS/GED             171\n          9-11               112\n          <9                  98\n(50, 60]  Some college/AA    258\n          College            220\n          HS/GED             220\n          9-11               122\n          <9                 104\n(60, 70]  Some college/AA    238\n          HS/GED             192\n          College            188\n          <9                 149\n          9-11               111\n(70, 80]  Some college/AA    217\n          HS/GED             184\n          <9                 164\n          College            156\n          9-11                88\n          Don't know           3\nName: DMDEDUC2x, dtype: int64\n\n\nWe can also stratify jointly by age and gender to explore how educational attainment varies by both of these factors simultaneously. In doing this, it is easier to interpret the results if we pivot the education levels into the columns, and normalize the counts so that they sum to 1. After doing this, the results can be interpreted as proportions or probabilities. One notable observation from this table is that for people up to age around 60, women are more likely to have graduated from college than men, but for people over aged 60, this relationship reverses.\n\n# Eliminate rare/missing values\ndx = da.loc[~da.DMDEDUC2x.isin([\"Don't know\", \"Missing\"]), :]  \n\n#group data\ndx = dx.groupby([\"agegrp\", \"RIAGENDRx\"])[\"DMDEDUC2x\"]\ndx = dx.value_counts()\ndx.head()\n\nagegrp    RIAGENDRx  DMDEDUC2x      \n(18, 30]  Female     Some college/AA    207\n                     College            156\n                     HS/GED             119\n                     9-11                44\n                     <9                  27\nName: DMDEDUC2x, dtype: int64\n\n\n\ndx = dx.unstack() # Restructure the results from 'long' to 'wide'\ndx.head()\n\n\n\n\n\n  \n    \n      \n      DMDEDUC2x\n      9-11\n      <9\n      College\n      HS/GED\n      Some college/AA\n    \n    \n      agegrp\n      RIAGENDRx\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      (18, 30]\n      Female\n      44\n      27\n      156\n      119\n      207\n    \n    \n      Male\n      55\n      20\n      122\n      118\n      157\n    \n    \n      (30, 40]\n      Female\n      42\n      46\n      149\n      78\n      159\n    \n    \n      Male\n      69\n      47\n      115\n      104\n      123\n    \n    \n      (40, 50]\n      Female\n      55\n      53\n      150\n      87\n      157\n    \n  \n\n\n\n\n\n# Normalize within each stratum to get proportions\ndx = dx.apply(lambda x: x/x.sum(), axis=1) \ndx.head()\n\n\n\n\n\n  \n    \n      \n      DMDEDUC2x\n      9-11\n      <9\n      College\n      HS/GED\n      Some college/AA\n    \n    \n      agegrp\n      RIAGENDRx\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      (18, 30]\n      Female\n      0.079566\n      0.048825\n      0.282098\n      0.215190\n      0.374322\n    \n    \n      Male\n      0.116525\n      0.042373\n      0.258475\n      0.250000\n      0.332627\n    \n    \n      (30, 40]\n      Female\n      0.088608\n      0.097046\n      0.314346\n      0.164557\n      0.335443\n    \n    \n      Male\n      0.150655\n      0.102620\n      0.251092\n      0.227074\n      0.268559\n    \n    \n      (40, 50]\n      Female\n      0.109562\n      0.105578\n      0.298805\n      0.173307\n      0.312749\n    \n  \n\n\n\n\n\nprint(dx.to_string(float_format=\"%.3f\"))  # Limit display to 3 decimal places\n\nDMDEDUC2x           9-11    <9  College  HS/GED  Some college/AA\nagegrp   RIAGENDRx                                              \n(18, 30] Female    0.080 0.049    0.282   0.215            0.374\n         Male      0.117 0.042    0.258   0.250            0.333\n(30, 40] Female    0.089 0.097    0.314   0.165            0.335\n         Male      0.151 0.103    0.251   0.227            0.269\n(40, 50] Female    0.110 0.106    0.299   0.173            0.313\n         Male      0.142 0.112    0.274   0.209            0.262\n(50, 60] Female    0.117 0.102    0.245   0.234            0.302\n         Male      0.148 0.123    0.231   0.242            0.256\n(60, 70] Female    0.118 0.188    0.195   0.206            0.293\n         Male      0.135 0.151    0.233   0.231            0.249\n(70, 80] Female    0.105 0.225    0.149   0.240            0.281\n         Male      0.113 0.180    0.237   0.215            0.255"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html",
    "href": "code/10_nhanes_univariate_practice.html",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "",
    "text": "This notebook will give you the opportunity to perform some univariate analyses on your own using the NHANES. These analyses are similar to what was done in the week 2 NHANES case study notebook.\nYou can enter your code into the cells that say “enter your code here”, and you can type responses to the questions into the cells that say “Type Markdown and Latex”.\nNote that most of the code that you will need to write below is very similar to code that appears in the case study notebook. You will need to edit code from that notebook in small ways to adapt it to the prompts below.\nTo get started, we will use the same module imports and read the data in the same way as we did in the case study:"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-1",
    "href": "code/10_nhanes_univariate_practice.html#question-1",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.1 Question 1",
    "text": "11.1 Question 1\nRelabel the marital status variable DMDMARTL to have brief but informative character labels. Then construct a frequency table of these values for all people, then for women only, and for men only. Then construct these three frequency tables using only people whose age is between 30 and 40.\n\n# relabel colum\nda['DMDMARTLx'] = da['DMDMARTL'].replace({1:'Married', 2:'Widowed', 3:'Divorced', 4:'Separated', 5:'Never married', 6:'Living with partner', 77:'Refused', 99:'Unknown' }).fillna('Missing')\nda['RIAGENDRx'] = da['RIAGENDR'].replace({1:'Male', 2:'Female'})\n\n#create freq table\nx = da['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                48.474281\nNever married          17.506539\nDivorced               10.095902\nLiving with partner     9.189189\nWidowed                 6.904969\nMissing                 4.551003\nSeparated               3.243243\nRefused                 0.034874\nName: DMDMARTLx, dtype: float64\n\n\n\n#freq table for women only\nx = da[da['RIAGENDR']==2]['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                43.783602\nNever married          17.473118\nDivorced               11.760753\nWidowed                 9.946237\nLiving with partner     8.803763\nMissing                 4.233871\nSeparated               3.965054\nRefused                 0.033602\nName: DMDMARTLx, dtype: float64\n\n\n\n#freq table for male only\nx = da[da['RIAGENDR']==1]['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                53.533889\nNever married          17.542588\nLiving with partner     9.604929\nDivorced                8.300109\nMissing                 4.893077\nWidowed                 3.624502\nSeparated               2.464661\nRefused                 0.036245\nName: DMDMARTLx, dtype: float64\n\n\n\n#freq table for all people, age 30-40\nage30_40 = da[(da['RIDAGEYR'] >= 30) & (da['RIDAGEYR'] <= 40)]\nx = age30_40['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                54.580897\nNever married          21.150097\nLiving with partner    13.937622\nDivorced                6.822612\nSeparated               2.923977\nWidowed                 0.487329\nRefused                 0.097466\nName: DMDMARTLx, dtype: float64\n\n\n\n#freq table for females, age 30-40\nx = age30_40[age30_40['RIAGENDR']==2]['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                53.571429\nNever married          21.804511\nLiving with partner    12.218045\nDivorced                8.646617\nSeparated               3.383459\nWidowed                 0.375940\nName: DMDMARTLx, dtype: float64\n\n\n\n#freq table for males, age 30-40\nx = age30_40[age30_40['RIAGENDR']==1]['DMDMARTLx'].value_counts()\nx / x.sum()*100\n\nMarried                55.668016\nNever married          20.445344\nLiving with partner    15.789474\nDivorced                4.858300\nSeparated               2.429150\nWidowed                 0.607287\nRefused                 0.202429\nName: DMDMARTLx, dtype: float64\n\n\nQ1a. Briefly comment on some of the differences that you observe between the distribution of marital status between women and men, for people of all ages.\nThere are less married women and that seems to be due to more women being divorced\nQ1b. Briefly comment on the differences that you observe between the distribution of marital status states for women between the overall population, and for women between the ages of 30 and 40.\nMore women between 30-40 are married compared to the whole population and this group as less rates of widowed women\nQ1c. Repeat part b for the men.\nMore man in their 30-4o live with a partner"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-2",
    "href": "code/10_nhanes_univariate_practice.html#question-2",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.2 Question 2",
    "text": "11.2 Question 2\nRestricting to the female population, stratify the subjects into age bands no wider than ten years, and construct the distribution of marital status within each age band. Within each age band, present the distribution in terms of proportions that must sum to 1.\n\n#subset df\nfemales = da[da['RIAGENDR'] == 2].copy()\n\n#stratify\nfemales['agegr'] = pd.cut(females['RIDAGEYR'], [18, 30, 40, 50, 60, 70, 80])\n\n#group data\ndf =females.groupby(\"agegr\")[\"DMDMARTLx\"].value_counts().unstack().fillna(0)\n\n#normalize\ndf = df.apply(lambda x : x/x.sum() * 100, axis = 1)\n\ndf\n\n\n\n\n\n  \n    \n      DMDMARTLx\n      Divorced\n      Living with partner\n      Married\n      Missing\n      Never married\n      Refused\n      Separated\n      Widowed\n    \n    \n      agegr\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      (18, 30]\n      1.806240\n      18.719212\n      25.944171\n      9.195402\n      42.528736\n      0.000000\n      1.806240\n      0.000000\n    \n    \n      (30, 40]\n      9.071730\n      12.025316\n      54.430380\n      0.000000\n      20.464135\n      0.000000\n      3.586498\n      0.421941\n    \n    \n      (40, 50]\n      13.745020\n      7.370518\n      57.370518\n      0.000000\n      12.549801\n      0.000000\n      6.573705\n      2.390438\n    \n    \n      (50, 60]\n      17.659574\n      6.808511\n      54.680851\n      0.000000\n      8.936170\n      0.212766\n      5.744681\n      5.957447\n    \n    \n      (60, 70]\n      19.274376\n      4.308390\n      48.072562\n      0.000000\n      8.616780\n      0.000000\n      4.988662\n      14.739229\n    \n    \n      (70, 80]\n      14.390244\n      0.731707\n      31.707317\n      0.000000\n      5.121951\n      0.000000\n      1.951220\n      46.097561\n    \n  \n\n\n\n\nQ2a. Comment on the trends that you see in this series of marginal distributions.\nWe see an increase in: divorce over age groups We see a decrease in the proportion of females living with a partner + women never married There is a big spike in marriages (up to 50%) from age group 18-30 to 30-40 and then a slow decline The largest group of widowed women is in the oldest age group\nQ2b. Repeat the construction for males.\n\n#subset df\nmales = da[da['RIAGENDR'] == 1].copy()\n\n#stratify\nmales['agegr'] = pd.cut(males['RIDAGEYR'], [18, 30, 40, 50, 60, 70, 80])\n\n#group data\ndf =males.groupby(\"agegr\")[\"DMDMARTLx\"].value_counts().unstack().fillna(0)\n\n#normalize\ndf = df.apply(lambda x : x/x.sum() * 100, axis = 1)\n\ndf\n\n\n\n\n\n  \n    \n      DMDMARTLx\n      Divorced\n      Living with partner\n      Married\n      Missing\n      Never married\n      Refused\n      Separated\n      Widowed\n    \n    \n      agegr\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      (18, 30]\n      0.367647\n      17.463235\n      19.117647\n      13.235294\n      48.161765\n      0.000000\n      1.286765\n      0.367647\n    \n    \n      (30, 40]\n      5.240175\n      15.720524\n      56.331878\n      0.000000\n      19.432314\n      0.218341\n      2.620087\n      0.436681\n    \n    \n      (40, 50]\n      8.478803\n      8.229426\n      70.324190\n      0.000000\n      9.725686\n      0.000000\n      2.743142\n      0.498753\n    \n    \n      (50, 60]\n      12.555066\n      7.488987\n      65.198238\n      0.000000\n      10.352423\n      0.000000\n      2.202643\n      2.202643\n    \n    \n      (60, 70]\n      12.585812\n      5.034325\n      66.590389\n      0.000000\n      8.695652\n      0.000000\n      3.203661\n      3.890160\n    \n    \n      (70, 80]\n      14.179104\n      2.238806\n      61.194030\n      0.000000\n      2.238806\n      0.000000\n      3.482587\n      16.666667\n    \n  \n\n\n\n\nQ2c. Comment on any notable differences that you see when comparing these results for females and for males.\nIncrease in divorce over time Decrease of males living with a partner and males that never married Largest increase in married males in group 30-40 and then slow decrease (but not to levels as for females) Separated relatively constant Largest increase in widowed men in the last group (but small compared to females)"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-3",
    "href": "code/10_nhanes_univariate_practice.html#question-3",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.3 Question 3",
    "text": "11.3 Question 3\nConstruct a histogram of the distribution of heights using the BMXHT variable in the NHANES sample.\n\nsns.histplot(da.BMXHT)\nplt.show()\n\n\n\n\nQ3a. Use the bins argument to distplot to produce histograms with different numbers of bins. Assess whether the default value for this argument gives a meaningful result, and comment on what happens as the number of bins grows excessively large or excessively small.\n\nsns.histplot(da.BMXHT, bins = 200)\nplt.show()\n\n\n\n\nThe value looks good\nQ3b. Make separate histograms for the heights of women and men, then make a side-by-side boxplot showing the heights of women and men.\n\ng = sns.FacetGrid(da, row = 'RIAGENDRx')\ng = g.map(plt.hist, \"BMXHT\")\nplt.show()\n\n\n\n\n\nsns.boxplot(y = da['BMXHT'], x = da['RIAGENDRx'])\nplt.show()\n\n\n\n\nQ3c. Comment on what features, if any are not represented clearly in the boxplots, and what features, if any, are easier to see in the boxplots than in the histograms.\nMales are larger than females (we can see this in both plots, however the median is easier to see in the boxplot). There are outliers on both ends (clearer in the boxplot)"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-4",
    "href": "code/10_nhanes_univariate_practice.html#question-4",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.4 Question 4",
    "text": "11.4 Question 4\nMake a boxplot showing the distribution of within-subject differences between the first and second systolic blood pressure measurents (BPXSY1 and BPXSY2).\n\nsns.boxplot(da[['BPXSY1', 'BPXSY2']])\nplt.show()\n\n\n\n\nQ4a. What proportion of the subjects have a lower SBP on the second reading compared to the first?\n\n# insert your code here\n\nQ4b. Make side-by-side boxplots of the two systolic blood pressure variables.\n\nsns.boxplot(da[['BPXSY1', 'BPXSY2']])\nplt.show()\n\n\n\n\n\nfig, ax =plt.subplots(1,2)\nsns.boxplot(da['BPXSY1'], ax = ax[0]).set_title(\"BPXSY1\")\nsns.boxplot(da['BPXSY2'], ax = ax[1]).set_title(\"BPXSY2\")\nplt.show()\n\n\n\n\nQ4c. Comment on the variation within either the first or second systolic blood pressure measurements, and the variation in the within-subject differences between the first and second systolic blood pressure measurements."
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-5",
    "href": "code/10_nhanes_univariate_practice.html#question-5",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.5 Question 5",
    "text": "11.5 Question 5\nConstruct a frequency table of household sizes for people within each educational attainment category (the relevant variable is DMDEDUC2). Convert the frequencies to proportions.\n\ndx = da.groupby([\"DMDEDUC2\"])[\"DMDHHSIZ\"].value_counts().unstack()\ndx = dx.apply(lambda x: x/x.sum(), axis=1)\ndx\n#print(dx.to_string(float_format=\"%.2f\")) \n\n\n\n\n\n  \n    \n      DMDHHSIZ\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n    \n      DMDEDUC2\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      0.109924\n      0.224427\n      0.146565\n      0.132824\n      0.148092\n      0.108397\n      0.129771\n    \n    \n      2.0\n      0.116641\n      0.222395\n      0.163297\n      0.152411\n      0.146190\n      0.113530\n      0.085537\n    \n    \n      3.0\n      0.152614\n      0.270658\n      0.171164\n      0.161889\n      0.109612\n      0.065767\n      0.068297\n    \n    \n      4.0\n      0.151141\n      0.268970\n      0.193091\n      0.169031\n      0.122147\n      0.050586\n      0.045034\n    \n    \n      5.0\n      0.142753\n      0.347731\n      0.193997\n      0.165447\n      0.095168\n      0.029283\n      0.025622\n    \n    \n      9.0\n      NaN\n      0.666667\n      NaN\n      NaN\n      0.333333\n      NaN\n      NaN\n    \n  \n\n\n\n\nQ5a. Comment on any major differences among the distributions.\nQ5b. Restrict the sample to people between 30 and 40 years of age. Then calculate the median household size for women and men within each level of educational attainment.\n\nda[(da.RIDAGEYR >= 30) & (da.RIDAGEYR <= 40)].groupby([\"DMDEDUC2\", \"RIAGENDR\"])[\"DMDHHSIZ\"].median()\n\nDMDEDUC2  RIAGENDR\n1.0       1           5.0\n          2           5.0\n2.0       1           4.5\n          2           5.0\n3.0       1           4.0\n          2           5.0\n4.0       1           4.0\n          2           4.0\n5.0       1           3.0\n          2           3.0\nName: DMDHHSIZ, dtype: float64"
  },
  {
    "objectID": "code/10_nhanes_univariate_practice.html#question-6",
    "href": "code/10_nhanes_univariate_practice.html#question-6",
    "title": "11  Practice notebook for univariate analysis using NHANES data",
    "section": "11.6 Question 6",
    "text": "11.6 Question 6\nThe participants can be clustered into “maked variance units” (MVU) based on every combination of the variables SDMVSTRA and SDMVPSU. Calculate the mean age (RIDAGEYR), height (BMXHT), and BMI (BMXBMI) for each gender (RIAGENDR), within each MVU, and report the ratio between the largest and smallest mean (e.g. for height) across the MVUs.\n\nda.groupby(['SDMVSTRA', 'SDMVPSU', 'RIAGENDR']) \\\n    [['RIDAGEYR', 'BMXHT', 'BMXBMI']] \\\n    .mean().unstack()\n\n\n\n\n\n  \n    \n      \n      \n      RIDAGEYR\n      BMXHT\n      BMXBMI\n    \n    \n      \n      RIAGENDR\n      1\n      2\n      1\n      2\n      1\n      2\n    \n    \n      SDMVSTRA\n      SDMVPSU\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      119\n      1\n      47.861111\n      47.663265\n      172.741667\n      159.570408\n      26.958333\n      30.052041\n    \n    \n      2\n      54.363636\n      52.987952\n      172.906818\n      159.244578\n      27.160465\n      27.849398\n    \n    \n      120\n      1\n      43.130000\n      43.636364\n      169.537755\n      155.402041\n      30.939175\n      32.419388\n    \n    \n      2\n      45.219178\n      43.736111\n      173.075342\n      159.218056\n      27.727397\n      27.400000\n    \n    \n      121\n      1\n      46.750000\n      44.397959\n      172.177885\n      158.871579\n      29.416505\n      30.856842\n    \n    \n      2\n      42.063158\n      44.376344\n      174.764516\n      160.229032\n      26.273118\n      26.470968\n    \n    \n      122\n      1\n      44.653061\n      42.897436\n      173.998969\n      161.315385\n      28.528866\n      29.447436\n    \n    \n      2\n      44.320000\n      47.333333\n      170.332323\n      157.231111\n      25.744444\n      26.611111\n    \n    \n      123\n      1\n      47.829787\n      44.841121\n      174.315217\n      162.059615\n      29.231522\n      29.905769\n    \n    \n      2\n      52.126582\n      46.457447\n      174.454430\n      160.476596\n      28.811392\n      30.641489\n    \n    \n      124\n      1\n      50.750000\n      51.664000\n      172.109009\n      158.788710\n      28.614414\n      29.533065\n    \n    \n      2\n      48.245614\n      42.541667\n      174.291228\n      162.853521\n      27.714035\n      28.640845\n    \n    \n      125\n      1\n      55.165289\n      50.900901\n      173.631092\n      160.762385\n      29.727731\n      30.385321\n    \n    \n      2\n      49.705882\n      51.660000\n      174.456863\n      160.021429\n      29.143564\n      28.564286\n    \n    \n      126\n      1\n      48.416667\n      46.229167\n      175.149398\n      160.387500\n      29.033333\n      31.262500\n    \n    \n      2\n      48.666667\n      47.205882\n      174.713043\n      160.892000\n      29.039130\n      29.612121\n    \n    \n      127\n      1\n      53.137931\n      49.694444\n      171.545349\n      157.422430\n      31.062353\n      32.189720\n    \n    \n      2\n      54.070588\n      51.486239\n      173.366667\n      159.022936\n      30.557831\n      30.770642\n    \n    \n      128\n      1\n      53.673267\n      55.638462\n      169.325000\n      156.339063\n      31.749000\n      32.303125\n    \n    \n      2\n      45.822785\n      45.589744\n      172.400000\n      160.437179\n      26.835443\n      27.491026\n    \n    \n      129\n      1\n      43.922222\n      45.329787\n      171.094318\n      156.900000\n      26.493182\n      29.019149\n    \n    \n      2\n      45.775510\n      43.500000\n      173.138298\n      161.034259\n      28.961702\n      29.429630\n    \n    \n      130\n      1\n      50.516854\n      47.810526\n      176.974157\n      161.977895\n      30.337079\n      30.700000\n    \n    \n      2\n      50.535354\n      50.833333\n      175.061224\n      160.060577\n      29.237755\n      31.490385\n    \n    \n      131\n      1\n      53.140187\n      54.893617\n      175.610476\n      161.989362\n      28.259615\n      30.061702\n    \n    \n      2\n      46.778846\n      45.000000\n      175.091346\n      161.673810\n      30.077885\n      32.984127\n    \n    \n      132\n      1\n      42.380435\n      43.210526\n      172.534066\n      161.508421\n      28.546154\n      29.848421\n    \n    \n      2\n      49.038760\n      51.700000\n      172.809524\n      159.138281\n      28.966667\n      30.540625\n    \n    \n      133\n      1\n      44.054795\n      45.105882\n      171.509722\n      158.295122\n      27.495833\n      27.959259\n    \n    \n      2\n      47.489796\n      47.063158\n      171.179167\n      158.627368\n      27.966667\n      29.000000\n    \n  \n\n\n\n\nQ6a. Comment on the extent to which mean age, height, and BMI vary among the MVUs.\nQ6b. Calculate the inter-quartile range (IQR) for age, height, and BMI for each gender and each MVU. Report the ratio between the largest and smalles IQR across the MVUs.\n\n# insert your code here\n\nQ6c. Comment on the extent to which the IQR for age, height, and BMI vary among the MVUs."
  }
]